---
title: "CVTDM Project - Wine Quality"
author: "Sam Blum and Mathis Da Silva"
date: "25/11/2021"
output: html_document
---

```{r, warning = FALSE, message = FALSE}
rm(list = ls())

library(naniar)
library(ggplot2)
library(reshape2)
library(magrittr)
library(dplyr)
library(car)
library(GGally)
library(viridis)
library(caret)
library(ggplot2)
library(cowplot)
library(FNN)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)
library(imbalance)
library(e1071)
library(neuralnet)
library(nnet)

# setwd("~/GitHub/CVTDM_Project")
wine = read.csv(file = "winequality-white.csv", header = T, sep = ";")
```

# Data Exploratory Analysis
## First Insights

First, let's have a look at the variables and their types:
```{r, warning = FALSE}
str(wine)
```

Now, let's create a *binned_quality* variable for the purpose of the problem:
```{r}
sapply(wine, function(x) length(unique(x))) 
wine$binned_quality = as.factor(ifelse(wine$quality < 5, 'Low',
                                ifelse(wine$quality >= 5 & wine$quality < 7, "Intermediate",
                                ifelse(wine$quality >= 7, "High", "None"))))

wine$quality = as.factor(wine$quality)
```

We can also have a look at the summary statistics:
```{r}
summary(wine)

sapply(wine[,-c(12,13)], sd)

str(wine)
```

## Visualisation & Distributions

Let's look at the distribution of the variables based on the quality variable (not binned, on a 0-10 scale):
```{r message=FALSE, warning=FALSE}
boxplots = ggplot(data = melt(wine[,-13], "quality"), aes(quality, value, group = quality)) + 
  geom_boxplot(fill = "transparent", color = "black") + 
  facet_wrap(~variable, scale = "free", ncol = 3) +
  theme_classic()

boxplots
```

First, one can see that there are not a lot of variation between wine of different quality. There is a slight increase in alcohol quantity for wine of better quality, as well as a slight increase in pH levels.

Let's continue by exploring the distribution of each variable:
```{r}
par(mfrow=c(2, 3))
for (i in 1:11) {
  hist(wine[, i], main = c("Histogram for", names(wine[i])), xlab=names(wine[i]))
  abline(v = mean(wine[, i]), col = 1, lwd = 2)
  abline(v = median(wine[, i]), col = 2, lwd = 2)
}

par(mfrow=c(1, 1))
barplot(table(wine$binned_quality), main = c("Histogram for quality", names(wine$binned_quality)), xlab=names(wine$binned_quality))
```

Our dataset faces two problems we have to deal with: imbalance between quality groups ('Intermediate' quality is over-represented) and skewness in the distribution of the explanatory variables.

To correct for the skewness, we can log-transform the variables of interest:
```{r, warning= FALSE}
alllogwine = wine
alllogwine[,-c(12,13)] = lapply(alllogwine[,-c(12,13)], log) #log transform all variables except quality and binned quality

boxplots = ggplot(data = melt(alllogwine[,-13], "quality"), aes(quality, value, group = quality)) + 
  geom_boxplot(fill = "transparent", color = "black") + 
  facet_wrap(~variable, scale = "free", ncol = 3) +
  theme_classic()

boxplots
```

Let's have a look at the distribution of the variables after log-tranformation:
```{r}
par(mfrow=c(2, 3))
for (i in 1:11) {
  hist(alllogwine[, i], main = c("Histogram for", names(alllogwine[i])), xlab=names(alllogwine[i]))
  abline(v = mean(alllogwine[, i]), col = 1, lwd = 2)
  abline(v = median(alllogwine[, i]), col = 2, lwd = 2)
}
```

Clearly, we see an improvement in the distribution of the variables, with less variability due to extreme observations. There are still some variables for which the transformation does not add much: *citric.acid*, *total.sulfure.dioxide* and *pH*. Indeed, these variables were already pretty well distributed. Also, note that density is still right-skewed. 

## Correlation Between Explanatory Variables

Now, let's  have a look at the correlation between the explanatory variables:
```{r, warning = FALSE}
cor_mat = round(cor(wine[,-c(12,13)]),2) 
cor_mat2 = melt(cor_mat)

ggplot(data = cor_mat2, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), color = "white", size = 3) +
  labs(title = "Heatmap of the correlation table") +
  theme(axis.text.x = element_text(angle=90))
```

We notice a high negative correlation coefficient between alcohol and density (-0.78). Because it is generally better to reduce the number of dimensions and because a high correlation might lead to multicollinearity issues, one can decide to drop the *density* variable.

Let's look at the VIFs (Variance Inflation Factors) by performing a linear regression:
```{r, warning = FALSE}
wine$quality = as.numeric(wine$quality)

model = lm(quality ~., data = wine[,-13])
vif(model)
```

It appears that the *density* variable has a very high VIF (28.2) compared to other variables. Hence, let's have a look at the model without *density* and see if there is an improvement:
```{r}
model2 = lm(quality ~., data = wine[,-c(8,13)])
vif(model2)

wine$quality = as.factor(wine$quality)
```

Clearly, the VIF of *alcohol* is lower compared to before (7.7 vs. 1.6). Thus, we will drop *density* from our models. 

Finally, let's look at this plot:
```{r}
ggparcoord(wine, columns = 1:11, groupColumn = 13, showPoints = TRUE, alphaLines = 0.3, scale = "uniminmax") + scale_color_viridis(discrete = TRUE) + theme(axis.text.x = element_text(angle = 90))
```

[... COMMENTS].

# Data pre-processing
## Variable selection and log-transformation

First, let's drop density and quality:
```{r, warning = FALSE}
logwine = wine[,-c(8,12)]
```

Now, let's log transform all variables except *citric.acid*, *total.sulfure.dioxide*, *pH* and *binned_quality*:
```{r, warning = FALSE}
logwine[,-c(3,7,8,11)] = lapply(logwine[,-c(3,7,8,11)], log) #log transform all variables except citric.acid, total.sulfure.dioxide, pH and binned_quality
head(logwine) 
```

## Data partitioning

We can now proceed to the partitioning of the data, with a training set (50%), validation set (30%) and a test set (20%):
```{r, warning = FALSE}
set.seed(1)
train_index = createDataPartition(logwine$binned_quality, p = .5, list = FALSE)
train_df = logwine[train_index,]
valid_test_df = logwine[-train_index,]
valid_index = createDataPartition(valid_test_df$binned_quality, p = .6, list = FALSE)
valid_df = valid_test_df[valid_index,]
test_df = valid_test_df[-valid_index,]
```

## Data normalization

Because we use some classification techniques that require the data to be on a same scale (kNN, Neural Nets and Logistic Regression), we normalize the data on a [0,1] scale:
```{r, warning = FALSE}
#initialize normalized training and validation data frames to the original ones
train_norm_df = train_df
valid_norm_df = valid_df
test_norm_df = test_df

#use PreProcess() from the caret package and predict() to normalize numerical variables
norm_values = preProcess(train_df[,-c(11)], method = "range")
train_norm_df[,-c(11)] = predict(norm_values, train_df[,-c(11)])
valid_norm_df[,-c(11)] = predict(norm_values, valid_df[,-c(11)])
test_norm_df[,-c(11)] = predict(norm_values, test_df[,-c(11)])
```

## Dummyfication of the outcome variable

Because we perform multi-label classification, and because the *neuralnet* package requires the outcome variable to be coded as (0, 1) vector, we create 3 dummies, one for every quality level ('High', 'Intermediate' and 'Low'). Note that we also apply this to validation and training sets:
```{r}
# Initialize normalized training and validation data frames with dummies to the normalized ones
train_norm_dummy_df <- train_norm_df
valid_norm_dummy_df <- valid_norm_df
test_norm_dummy_df <- test_norm_df

# Creation of the vectors and implementation
train_norm_dummy_df$high_quality <- ifelse(train_norm_dummy_df$binned_quality == "High", 1, 0)
train_norm_dummy_df$intermediate_quality <- ifelse(train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
train_norm_dummy_df$low_quality <- ifelse(train_norm_dummy_df$binned_quality == "Low", 1, 0)

valid_norm_dummy_df$high_quality <- ifelse(valid_norm_dummy_df$binned_quality == "High", 1, 0)
valid_norm_dummy_df$intermediate_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
valid_norm_dummy_df$low_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Low", 1, 0)

test_norm_dummy_df$high_quality <- ifelse(test_norm_dummy_df$binned_quality == "High", 1, 0)
test_norm_dummy_df$intermediate_quality <- ifelse(test_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
test_norm_dummy_df$low_quality <- ifelse(test_norm_dummy_df$binned_quality == "Low", 1, 0)
```

## Oversampling

As stated earlier in the Data Exploratory Analysis, we face an imbalanced dataset:
```{r}
table(logwine$binned_quality)
prop.table(table(logwine$binned_quality))
```

We see that 'Intermediate' quality represents 74.6% of observations, whereas 'High' and 'Low' quality only represent 21.6% and 3.7% of observations (respectively). Thus, the classification models could have problems in predicting 'High', and even more importantly 'Low' observations, as they only represent 3.7% of observations.

To solve this problem, we can oversample the 'Low' observations and add them into the training set. We use Random Walk Oversampling, as this method is known to be one of the most effective, especially when facing such an imbalanced dataset (Huaxiang Zhang, Mingfang Li, 2014). 

Now, let's create 1736 new instances for 'Low' and 1298 instances for 'High' quality (such that we have the same number of observations in each category within the training set) and let's add them to the training set:
```{r, warning = FALSE}
set.seed(1)
add_low_train_df = rwo(train_df, 1736, "binned_quality") # Generation of 1736 instances for 'Low'
os_train_df = rbind(train_df, add_low_train_df) # Combining the new instances to the training set
set.seed(1)
add_high_train_df = rwo(os_train_df, 1298, "binned_quality") # Generation of 1298 instances for 'High'
os_train_df = rbind(os_train_df, add_high_train_df)
```

Let's normalize the newly generated observations, and add them to the normalize training set:
```{r, warning = FALSE}
os_train_norm_df = os_train_df

#use PreProcess() from the caret package and predict() to normalize numerical variables
os_norm_values = preProcess(os_train_df[,-c(11)], method = "range")
os_train_norm_df[,-c(11)] = predict(os_norm_values, os_train_df[,-c(11)])
```

We can also add these newly generated observations to the normalized training set with dummies for quality (used for Neural Nets):
```{r}
# Initialize oversampled normalized training set with dummies to the oversampled normalized one
os_train_norm_dummy_df <- os_train_norm_df

os_train_norm_dummy_df$high_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "High", 1, 0)
os_train_norm_dummy_df$intermediate_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
os_train_norm_dummy_df$low_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Low", 1, 0)
```

# Analysis

In this part, we will proceed to the analysis using the following classification techniques: kNN, Ordinal Logistic Regression, Naive Bayes, Classification Trees (fitted to CP, Boosted Tree, Bagged Tree and Random Forest) and Neural Nets. Finally, we will combine the results to produce two ensemble methods based on Majority Voting and Mean Probability rules.

Note that we will use the training sets with oversampling on 'Low' quality observations for all methods.

## kNN

[COMMENTS ON METHODOLOGY]

Let's proceed to the kNN:
```{r, warning = FALSE}
#initialize a new data frame with three columns: k, kappa, and balanced_accuracy
best_k_df = data.frame(k = seq(1, 50, 1), kappa = rep(0,50), balanced_accuracy = rep(0,50))

#perform knn on the validation set using different k then store accuracy and balanced accuracy for each k in the data frame
for(i in 1:50) {
  knn_pred = knn(train = os_train_norm_df[,-11], test = valid_norm_df[,-11], cl = os_train_norm_df[,11], k = i)
  best_k_df[i, 2] = confusionMatrix(knn_pred, valid_norm_df[,11])$overall[2]
  low_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[3,1]
  intermediate_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[2,1]
  high_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[1,1]
  best_k_df[i, 3] = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
}

kappa_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=kappa), color="red") + theme_classic()
balanced_accuray_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=balanced_accuracy), color="blue") + theme_classic()

plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")

which.max(best_k_df$kappa)#best k based on kappa
which.max(best_k_df$balanced_accuracy)#best k based on balanced accuracy
```

From the plots above, it seems that the best number of neighbours (i.e., number of k's) is k=1 if we want to maximize the balanced accuracy and the kappa value. 

Choosing this parameter(k=1), we can assess the predictive performance of the kNN model on the test set:
```{r, warning = FALSE}
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = os_train_norm_df[,-11], test = test_norm_df[,-11], cl = os_train_norm_df[,11], k = 1, prob = T)
best_knn_cm <- confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix 
best_knn_cm

kappa_best_knn = best_knn_cm$overall[2]
kappa_best_knn

low_sensitivity = best_knn_cm$byClass[3,1]
intermediate_sensitivity = best_knn_cm$byClass[2,1]
high_sensitivity = best_knn_cm$byClass[1,1]

bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
```

[COMMENTS ON RESULTS]

## Logistic regression 

[COMMENTS ON METHODOLOGY]

```{r, warning = FALSE}
os_log_reg = multinom(binned_quality ~ ., data = os_train_df)
summary(os_log_reg)
```

```{r, warning = FALSE}
os_log_prob = predict(os_log_reg, newdata = test_df, type = "p")

os_log_pred = data.frame("pred" = colnames(os_log_prob)[apply(os_log_prob,1,which.max)])
os_log_pred$pred = as.factor(os_log_pred$pred)

os_log_cm <- confusionMatrix(os_log_pred$pred, test_df[,11])
os_log_cm

kappa_logist = os_log_cm$overall[2]
kappa_logist

low_sensitivity = os_log_cm$byClass[3,1]
intermediate_sensitivity = os_log_cm$byClass[2,1]
high_sensitivity = os_log_cm$byClass[1,1]

bal_acc_logist = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_logist
```

[COMMENTS ON RESULTS]

## Naive Bayes

[COMMENTS ON METHODOLOGY]

```{r, warning = FALSE}
os_nb_model = naiveBayes (binned_quality ~ ., data = os_train_df)
os_nb_model
```

```{r, warning = FALSE}
os_nb_pred = predict(os_nb_model, newdata = test_df, type = "class")
os_nb_prob = predict(os_nb_model, newdata = test_df, type = "raw")
bayes_cm <- confusionMatrix(os_nb_pred, test_df[,11])
bayes_cm

kappa_bayes = bayes_cm$overall[2]
kappa_bayes

low_sensitivity = bayes_cm$byClass[3,1]
intermediate_sensitivity = bayes_cm$byClass[2,1]
high_sensitivity = bayes_cm$byClass[1,1]

bal_acc_bayes = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_bayes
```

[COMMENTS ON RESULTS]

## Classification trees
### Regular Classification Tree

Let's first create a fully grown tree before pruning it to the best CP (i.e., CP associated with the minimum standard error):
```{r}
set.seed(1)
tree1 <- rpart(binned_quality ~ ., data=os_train_df, method = "class", control = rpart.control(cp = 0, minsplit = 1, minbucket = 1, xval = 5))

# Look at the minimum standard error
printcp(tree1)
which.min(tree1$cptable[, 4])

# Take the cp associated with the minimum standard error to prune the tree:
set.seed(1)
pruned_tree1 <- prune(tree1, cp = tree1$cptable[which.min(tree1$cptable[, "xerror"]), "CP"])
prp(pruned_tree1)

# If we want to look at the most important variables:
pruned_tree1$variable.importance

# We can now predict the outcome:
pruned_tree1_pred_test <- predict(pruned_tree1, test_df, type="class")
pruned_tree1_prob_test <- predict(pruned_tree1, test_df, type="prob")
pruned_tree1_cm <- confusionMatrix(pruned_tree1_pred_test, test_df$binned_quality)
pruned_tree1_cm

kappa_pruned_tree1 = pruned_tree1_cm$overall[2]
kappa_pruned_tree1

low_sensitivity = pruned_tree1_cm$byClass[3,1]
intermediate_sensitivity = pruned_tree1_cm$byClass[2,1]
high_sensitivity = pruned_tree1_cm$byClass[1,1]
bal_acc_pruned_tree1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_pruned_tree1
```

### Boosted tree

Let's proceed to the Boosted Tree:
```{r}
set.seed(1)
boost1 <- boosting(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
boost1_pred_test <- predict(boost1, test_df, type="class")
boost1_prob_test <- predict(boost1, test_df, type="prob")
boost1_cm <- confusionMatrix(as.factor(boost1_pred_test$class), test_df$binned_quality)
boost1_cm

kappa_boost1 = boost1_cm$overall[2]
kappa_boost1

low_sensitivity = boost1_cm$byClass[3,1]
intermediate_sensitivity = boost1_cm$byClass[2,1]
high_sensitivity = boost1_cm$byClass[1,1]
bal_acc_boost1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_boost1
```

### Bagged tree

Let's proceed to the Bagged Tree:
```{r}
set.seed(1)
bag1 <- bagging(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
bag1_pred_test <- predict(bag1, test_df, type="class")
bag1_prob_test <- predict(bag1, test_df, type="prob")
bag1_cm <- confusionMatrix(as.factor(bag1_pred_test$class), test_df$binned_quality)
bag1_cm

kappa_bag1 = bag1_cm$overall[2]
kappa_bag1

low_sensitivity = bag1_cm$byClass[3,1]
intermediate_sensitivity = bag1_cm$byClass[2,1]
high_sensitivity = bag1_cm$byClass[1,1]
bal_acc_bag1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_bag1
```

### Random Forest

Let's proceed to the Random Forest:
```{r}
set.seed(1)
rf1 <- randomForest(os_train_df[-11], os_train_df$binned_quality, control = rpart.control(xval = 5))
rf1_pred_test <- predict(rf1, test_df, type="class")
rf1_prob_test <- predict(rf1, test_df, type="prob")
rf1_cm <- confusionMatrix(rf1_pred_test, test_df$binned_quality)
rf1_cm

kappa_rf1 = rf1_cm$overall[2]
kappa_rf1

low_sensitivity = rf1_cm$byClass[3,1]
intermediate_sensitivity = rf1_cm$byClass[2,1]
high_sensitivity = rf1_cm$byClass[1,1]
bal_acc_rf1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_rf1
```

## Neural Nets

As stated earlier in the Data Exploratory Analysis, we will use the neuralnet package. To do so, we will use the oversampled and normalized training set that contains dummies for the outcome variable.

In order to choose the best number of nodes, we will use a loop (2 to 10 nodes) and look at the performance of the model on the validation set (based on the kappa value). We will repeat this operation twice, once with a threshold of 0.1, and once with a threshold of 0.2.

### Determination of the best Neural Net with a threshold of 0.5

```{r}
nn_nodes_1 <- data.frame("number_nodes" = seq(from= 2, to = 10), "kappa_value" = NA, "balanced_accuracy" = NA)

for (i in 2:10){
  set.seed(1)
  nn <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = os_train_norm_dummy_df[, -11], hidden = i, linear.output = FALSE, threshold = 0.5)
  valid_pred_nn <- data.frame(predict(nn, valid_norm_dummy_df[, -c(11, 12, 13, 14)]))
  names(valid_pred_nn)[1:3] <- c("High", "Intermediate", "Low")
  valid_pred_nn$Prediction <- NA

  for(j in 1:length(valid_pred_nn$High)) {
  valid_pred_nn[j, 4] <- names(which.max(valid_pred_nn[j, 1:3]))
}
  
  nn.cm <- confusionMatrix(as.factor(valid_pred_nn$Prediction),valid_norm_df$binned_quality)
  nn_nodes_1[i-1, 2] <- nn.cm[["overall"]][["Kappa"]]
  nn_nodes_1[i-1, 3] <- (nn.cm$byClass[3,1] + nn.cm$byClass[2,1] + nn.cm$byClass[1,1]) / 3
}

nn_nodes_1
```

### Best Neural Net

Best Neural Net is with threshold = 0.5 and 4 nodes:

```{r}
set.seed(1)
nn1 <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = train_norm_dummy_df[, -11], hidden = 4, linear.output = FALSE, threshold = 0.5)

test_pred_nn1 <- data.frame(predict(nn1, test_norm_dummy_df[, -c(11, 12, 13, 14)]))
names(test_pred_nn1)[1:3] <- c("High", "Intermediate", "Low")
test_pred_nn1$Prediction <- NA

for(j in 1:length(test_pred_nn1$High)) {
test_pred_nn1[j, 4] <- names(which.max(test_pred_nn1[j, 1:3]))
}

nn1_cm <- confusionMatrix(as.factor(test_pred_nn1$Prediction), test_norm_df$binned_quality)
nn1_cm

kappa_nn1 = nn1_cm$overall[2]
kappa_nn1

low_sensitivity = nn1_cm$byClass[3,1]
intermediate_sensitivity = nn1_cm$byClass[2,1]
high_sensitivity = nn1_cm$byClass[1,1]
bal_acc_nn1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_nn1
```

### Comparing results

```{r, warning = FALSE}
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
                        "kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
                        "balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
```

```{r, warning = FALSE}
all_kappa_plot = ggplot(data = results_df, aes(x = reorder(method, kappa))) + 
  geom_bar(aes(y = kappa), stat = "identity") +
  labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
  theme(axis.text.x = element_text(angle=90))

all_bal_acc_plot = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) + 
  geom_bar(aes(y = balanced_accuracy), stat = "identity") +
  labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
  theme(axis.text.x = element_text(angle=90))

plot_grid(all_kappa_plot, all_bal_acc_plot)
```

### Ensembles

```{r, warning = FALSE}
ensemble_prob_low_df = data.frame("knn_prob_low" = attr(best_knn_pred, "prob")[,3],
                     "log_prob_low" = os_log_prob[,3],
                     "nb_prob_low" = os_nb_prob[,3],
                     "tree_prob_low" = pruned_tree1_prob_test[,3],
                     "boosting_prob_low" = boost1_prob_test$prob[,3],
                     "bagging_prob_low" = bag1_prob_test$prob[,3],
                     "rf_prob_low" = rf1_prob_test[,3],
                     "nnet_prob_low" = test_pred_nn1[,3])
ensemble_prob_low_df$mean_prob_low = apply(ensemble_prob_low_df[,c(1,2,5,7)], 1, mean)
head(ensemble_prob_low_df)

ensemble_prob_inter_df = data.frame("knn_prob_inter" = attr(best_knn_pred, "prob")[,2],
                     "log_prob_inter" = os_log_prob[,2],
                     "nb_prob_inter" = os_nb_prob[,2],
                     "tree_prob_inter" = pruned_tree1_prob_test[,2],
                     "boosting_prob_inter" = boost1_prob_test$prob[,2],
                     "bagging_prob_inter" = bag1_prob_test$prob[,2],
                     "rf_prob_inter" = rf1_prob_test[,2],
                     "nnet_prob_inter" = test_pred_nn1[,2])
ensemble_prob_inter_df$mean_prob_inter = apply(ensemble_prob_inter_df[,c(1,2,5,7)], 1, mean)
head(ensemble_prob_inter_df)

ensemble_prob_high_df = data.frame("knn_prob_high" = attr(best_knn_pred, "prob")[,1],
                     "log_prob_high" = os_log_prob[,1],
                     "nb_prob_high" = os_nb_prob[,1],
                     "tree_prob_high" = pruned_tree1_prob_test[,1],
                     "boosting_prob_high" = boost1_prob_test$prob[,1],
                     "bagging_prob_high" = bag1_prob_test$prob[,1],
                     "rf_prob_high" = rf1_prob_test[,1],
                     "nnet_prob_high" = test_pred_nn1[,1])
ensemble_prob_high_df$mean_prob_high = apply(ensemble_prob_high_df[,c(1,2,5,7)], 1, mean)
head(ensemble_prob_high_df)

ensemble_prob_df = data.frame("Low" = ensemble_prob_low_df$mean_prob_low,
                              "Intermediate" = ensemble_prob_inter_df$mean_prob_inter,
                              "High" = ensemble_prob_high_df$mean_prob_high)
ensemble_prob_df$mean_prob_pred = as.factor(colnames(ensemble_prob_df)[apply(ensemble_prob_df,1,which.max)])
head(ensemble_prob_df)
```

```{r, warning = FALSE}
ensembles_pred_df = data.frame("actual_value" = test_df$binned_quality,
                              "knn_pred" = best_knn_pred,
                              "log_pred" = os_log_pred$pred,
                              "nb_pred" = os_nb_pred,
                              "tree_pred" = pruned_tree1_pred_test,
                              "boosting_pred" = as.factor(boost1_pred_test$class),
                              "bagging_pred" = as.factor(bag1_pred_test$class),
                              "rf_pred" = rf1_pred_test, 
                              "nnet_pred" = as.factor(test_pred_nn1$Prediction))
ensembles_pred_df$majority_vote_pred = as.factor(apply(ensembles_pred_df[,c(2,3,4,6,8)], 1, function(x) names(which.max(table(x)))))
ensembles_pred_df$mean_prob_pred = ensemble_prob_df$mean_prob_pred
head(ensembles_pred_df)
```

```{r, warning = FALSE}
majority_vote_cm <- confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])
majority_vote_cm

kappa_majority = majority_vote_cm$overall[2]
kappa_majority

low_sensitivity = majority_vote_cm$byClass[3,1]
intermediate_sensitivity = majority_vote_cm$byClass[2,1]
high_sensitivity = majority_vote_cm$byClass[1,1]

bal_acc_majority = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_majority
```

```{r, warning = FALSE}
mean_prob_cm <- confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])
mean_prob_cm

kappa_mean_prob = mean_prob_cm$overall[2]
kappa_mean_prob

low_sensitivity = mean_prob_cm$byClass[3,1]
intermediate_sensitivity = mean_prob_cm$byClass[2,1]
high_sensitivity = mean_prob_cm$byClass[1,1]

bal_acc_mean_prob = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_mean_prob
```

### Comparing results (including Ensembles)

```{r, warning = FALSE}
results_df[9,] = list("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = list("mean probability", kappa_mean_prob, bal_acc_mean_prob)
```

```{r, warning = FALSE}
all_kappa_plot2 = ggplot(data = results_df, aes(x = reorder(method, kappa))) + 
  geom_bar(aes(y = kappa), stat = "identity") +
  labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
  theme(axis.text.x = element_text(angle=90))

all_bal_acc_plot2 = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) + 
  geom_bar(aes(y = balanced_accuracy), stat = "identity") +
  labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
  theme(axis.text.x = element_text(angle=90))

plot_grid(all_kappa_plot2, all_bal_acc_plot2)
```

### Visualising Confusion Matrices for each method

We first create the data-frames associated to the confusion matrices of each method:
```{r}
plot_knn_cm <- as.data.frame(best_knn_cm$table)
plot_knn_cm$Prediction <- factor(plot_knn_cm$Prediction, levels=rev(levels(plot_knn_cm$Prediction)))
plot_knn_cm <- plot_knn_cm %>%
  mutate(pred = ifelse(plot_knn_cm$Prediction == plot_knn_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))


plot_os_log_cm <- as.data.frame(os_log_cm$table)
plot_os_log_cm$Prediction <- factor(plot_os_log_cm$Prediction, levels=rev(levels(plot_os_log_cm$Prediction)))
plot_os_log_cm <- plot_os_log_cm %>%
  mutate(pred = ifelse(plot_os_log_cm$Prediction == plot_os_log_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_bayes_cm <- as.data.frame(bayes_cm$table)
plot_bayes_cm$Prediction <- factor(plot_bayes_cm$Prediction, levels=rev(levels(plot_bayes_cm$Prediction)))
plot_bayes_cm <- plot_bayes_cm %>%
  mutate(pred = ifelse(plot_bayes_cm$Prediction == plot_bayes_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_pruned_tree1_cm <- as.data.frame(pruned_tree1_cm$table)
plot_pruned_tree1_cm$Prediction <- factor(plot_pruned_tree1_cm$Prediction, levels=rev(levels(plot_pruned_tree1_cm$Prediction)))
plot_pruned_tree1_cm <- plot_pruned_tree1_cm %>%
  mutate(pred = ifelse(plot_pruned_tree1_cm$Prediction == plot_pruned_tree1_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_boost1_cm <- as.data.frame(boost1_cm$table)
plot_boost1_cm$Prediction <- factor(plot_boost1_cm$Prediction, levels=rev(levels(plot_boost1_cm$Prediction)))
plot_boost1_cm <- plot_boost1_cm %>%
  mutate(pred = ifelse(plot_boost1_cm$Prediction == plot_boost1_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_bag1_cm <- as.data.frame(bag1_cm$table)
plot_bag1_cm$Prediction <- factor(plot_bag1_cm$Prediction, levels=rev(levels(plot_bag1_cm$Prediction)))
plot_bag1_cm <- plot_bag1_cm %>%
  mutate(pred = ifelse(plot_bag1_cm$Prediction == plot_bag1_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_rf1_cm <- as.data.frame(rf1_cm$table)
plot_rf1_cm$Prediction <- factor(plot_rf1_cm$Prediction, levels=rev(levels(plot_rf1_cm$Prediction)))
plot_rf1_cm <- plot_rf1_cm %>%
  mutate(pred = ifelse(plot_rf1_cm$Prediction == plot_rf1_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_nn1_cm <- as.data.frame(nn1_cm$table)
plot_nn1_cm$Prediction <- factor(plot_nn1_cm$Prediction, levels=rev(levels(plot_nn1_cm$Prediction)))
plot_nn1_cm <- plot_nn1_cm %>%
  mutate(pred = ifelse(plot_nn1_cm$Prediction == plot_nn1_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_majority_vote_cm <- as.data.frame(majority_vote_cm$table)
plot_majority_vote_cm$Prediction <- factor(plot_majority_vote_cm$Prediction, levels=rev(levels(plot_majority_vote_cm$Prediction)))
plot_majority_vote_cm <- plot_majority_vote_cm %>%
  mutate(pred = ifelse(plot_majority_vote_cm$Prediction == plot_majority_vote_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

plot_mean_prob_cm <- as.data.frame(mean_prob_cm$table)
plot_mean_prob_cm$Prediction <- factor(plot_mean_prob_cm$Prediction, levels=rev(levels(plot_mean_prob_cm$Prediction)))
plot_mean_prob_cm <- plot_mean_prob_cm %>%
  mutate(pred = ifelse(plot_mean_prob_cm$Prediction == plot_mean_prob_cm$Reference, "correct", "error")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))
```

Let's create the plots:
```{r}
plot_knn_cm <- ggplot(plot_knn_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
        geom_tile() + 
        theme_bw() + 
        geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
        scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
        xlim((levels(plot_knn_cm$Reference))) +
        labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: kNN")

plot_os_log_cm <- ggplot(plot_os_log_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_os_log_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Logistic Regression")

plot_bayes_cm <- ggplot(plot_bayes_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_bayes_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Naive Bayes")

plot_pruned_tree1_cm <- ggplot(plot_pruned_tree1_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_pruned_tree1_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Pruned Tree")

plot_boost1_cm <- ggplot(plot_boost1_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_boost1_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Boosted Tree")

plot_bag1_cm <- ggplot(plot_bag1_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_bag1_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Bagged Tree")

plot_rf1_cm <- ggplot(plot_rf1_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_rf1_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Random Forest")

plot_nn1_cm <- ggplot(plot_nn1_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_nn1_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Neural Net")

plot_majority_vote_cm <- ggplot(plot_majority_vote_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_majority_vote_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Majority Vote (Ensembles)")

plot_mean_prob_cm <- ggplot(plot_mean_prob_cm, aes(Reference, Prediction, fill = pred, alpha = prop)) +
    geom_tile() + 
    theme_bw() + 
    geom_text(aes(label=Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(correct = "#5DADE2", error = "#EC7063")) +
    xlim((levels(plot_mean_prob_cm$Reference))) +
    labs(x = "Reference",y = "Prediction", title = "Confusion Matrix: Mean Probability (Ensembles)")

plot_grid(plot_knn_cm, plot_os_log_cm, plot_bayes_cm, plot_pruned_tree1_cm, plot_boost1_cm, plot_bag1_cm, plot_rf1_cm, plot_nn1_cm, plot_majority_vote_cm, plot_mean_prob_cm)
```
