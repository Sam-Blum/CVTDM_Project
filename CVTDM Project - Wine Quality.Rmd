---
title: "CVTDM Project - Wine Quality"
author: "Sam Blum and Mathis Da Silva"
date: "25/11/2021"
output: html_document
---

```{r, warning = FALSE, message = FALSE}
rm(list = ls())

library(naniar)
library(ggplot2)
library(reshape2)
library(magrittr)
library(dplyr)
library(car)
library(GGally)
library(viridis)
library(caret)
library(ggplot2)
library(cowplot)
library(FNN)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)
library(imbalance)
library(e1071)
library(neuralnet)

# setwd("~/GitHub/CVTDM_Project")
wine = read.csv(file = "winequality-white.csv", header = T, sep = ";")
```

### Data Exploration and Pre-Processing

```{r, warning = FALSE}
str(wine)
```

```{r}
# We create the new binned quality variable:
sapply(wine, function(x) length(unique(x))) 
wine$binned_quality = as.factor(ifelse(wine$quality < 5, 'Low',
                                ifelse(wine$quality >= 5 & wine$quality < 7, "Intermediate",
                                ifelse(wine$quality >= 7, "High", "None"))))

wine$quality = as.factor(wine$quality)
```

```{r}
# Summary statistics and overview of the data with the new quality variable
summary(wine)

sapply(wine[,-c(12,13)], sd)

str(wine)
```

```{r message=FALSE, warning=FALSE}
# [...]
boxplots = ggplot(data = melt(wine[,-13], "quality"), aes(quality, value, group = quality)) + 
  geom_boxplot(fill = "transparent", color = "black") + 
  facet_wrap(~variable, scale = "free", ncol = 3) +
  theme_classic()

boxplots
```

```{r}
# Histograms for each variable
par(mfrow=c(2, 3))
for (i in 1:11) {
  hist(wine[, i], main = c("Histogram for", names(wine[i])), xlab=names(wine[i]))
  abline(v = mean(wine[, i]), col = 1, lwd = 2)
  abline(v = median(wine[, i]), col = 2, lwd = 2)
}

#Barplot for quality
par(mfrow=c(1, 1))
barplot(table(wine$binned_quality), main = c("Histogram for quality", names(wine$binned_quality)), xlab=names(wine$binned_quality))
```

```{r, warning= FALSE}
alllogwine = wine
alllogwine[,-c(12,13)] = lapply(alllogwine[,-c(12,13)], log) #log transform all variables except quality and binned quality

boxplots = ggplot(data = melt(alllogwine[,-13], "quality"), aes(quality, value, group = quality)) + 
  geom_boxplot(fill = "transparent", color = "black") + 
  facet_wrap(~variable, scale = "free", ncol = 3) +
  theme_classic()

boxplots
```

```{r}
# Gives histogram for log
par(mfrow=c(2, 3))
for (i in 1:11) {
  hist(alllogwine[, i], main = c("Histogram for", names(alllogwine[i])), xlab=names(alllogwine[i]))
  abline(v = mean(alllogwine[, i]), col = 1, lwd = 2)
  abline(v = median(alllogwine[, i]), col = 2, lwd = 2)
}
```

```{r, warning = FALSE}
# 
cor_mat = round(cor(wine[,-c(12,13)]),2) 
cor_mat2 = melt(cor_mat)

ggplot(data = cor_mat2, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), color = "white", size = 3) +
  labs(title = "Heatmap of the correlation table") +
  theme(axis.text.x = element_text(angle=90))
```

```{r, warning = FALSE}
wine$quality = as.numeric(wine$quality)

model = lm(quality ~., data = wine[,-13])
vif(model)

model2 = lm(quality ~., data = wine[,-c(8,13)])
vif(model2)

wine$quality = as.factor(wine$quality)
```

```{r}
ggparcoord(wine, columns = 1:11, groupColumn = 13, showPoints = TRUE, alphaLines = 0.3, scale = "uniminmax") + scale_color_viridis(discrete = TRUE) + theme(axis.text.x = element_text(angle = 90))
```

### Data pre-processing

```{r, warning = FALSE}
logwine = wine[,-c(8,12)] #drop density and quality
logwine[,-c(3,7,8,11)] = lapply(logwine[,-c(3,7,8,11)], log) #log transform all variables except citric.acid, total.sulfure.dioxide, pH and binned_quality
head(logwine) 
```

### Data partitioning

```{r, warning = FALSE}
set.seed(1)
train_index = createDataPartition(logwine$binned_quality, p = .5, list = FALSE)
train_df = logwine[train_index,]
valid_test_df = logwine[-train_index,]
valid_index = createDataPartition(valid_test_df$binned_quality, p = .6, list = FALSE)
valid_df = valid_test_df[valid_index,]
test_df = valid_test_df[-valid_index,]
```

### Data normalization

```{r, warning = FALSE}
#initialize normalized training and validation data frames to the original ones
train_norm_df = train_df
valid_norm_df = valid_df
test_norm_df = test_df

#use PreProcess() from the caret package and predict() to normalize numerical variables
norm_values = preProcess(train_df[,-c(11)], method = "range")
train_norm_df[,-c(11)] = predict(norm_values, train_df[,-c(11)])
valid_norm_df[,-c(11)] = predict(norm_values, valid_df[,-c(11)])
test_norm_df[,-c(11)] = predict(norm_values, test_df[,-c(11)])
```

### Dummyfication of the outcome variable (for using Neural Nets):
```{r}
# Need to dummify the outcome variable (because of the neuralnet package):
train_norm_dummy_df <- train_norm_df
valid_norm_dummy_df <- valid_norm_df
test_norm_dummy_df <- test_norm_df

train_norm_dummy_df$low_quality <- ifelse(train_norm_dummy_df$binned_quality == "Low", 1, 0)
train_norm_dummy_df$intermediate_quality <- ifelse(train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
train_norm_dummy_df$high_quality <- ifelse(train_norm_dummy_df$binned_quality == "High", 1, 0)

valid_norm_dummy_df$low_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Low", 1, 0)
valid_norm_dummy_df$intermediate_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
valid_norm_dummy_df$high_quality <- ifelse(valid_norm_dummy_df$binned_quality == "High", 1, 0)

test_norm_dummy_df$low_quality <- ifelse(test_norm_dummy_df$binned_quality == "Low", 1, 0)
test_norm_dummy_df$intermediate_quality <- ifelse(test_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
test_norm_dummy_df$high_quality <- ifelse(test_norm_dummy_df$binned_quality == "High", 1, 0)
```

### Oversampling
```{r, warning = FALSE}
set.seed(1)
add_train_df = rwo(train_df, 400, "binned_quality")
os_train_df = rbind(train_df, add_train_df)
```

```{r, warning = FALSE}
os_train_norm_df = os_train_df

#use PreProcess() from the caret package and predict() to normalize numerical variables
os_norm_values = preProcess(os_train_df[,-c(11)], method = "range")
os_train_norm_df[,-c(11)] = predict(os_norm_values, os_train_df[,-c(11)])
```

### Dummyfication of the outcome variable (for using Neural Nets **with oversampling**):
```{r}
# Need to dummify the outcome variable on the oversampled training set:
os_train_norm_dummy_df <- os_train_norm_df

os_train_norm_dummy_df$low_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Low", 1, 0)
os_train_norm_dummy_df$intermediate_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
os_train_norm_dummy_df$high_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "High", 1, 0)
```

### KNN

```{r, warning = FALSE}
#initialize a new data frame with three columns: k, accuracy, and balanced_accuracy
best_k_df = data.frame(k = seq(1, 50, 1), kappa = rep(0,50), balanced_accuracy = rep(0,50))

#perform knn on the validation set using different k then store accuracy and balanced accuracy for each k in the data frame
for(i in 1:50) {
  knn_pred = knn(train = train_norm_df[,-11], test = valid_norm_df[,-11], cl = train_norm_df[,11], k = i)
  best_k_df[i, 2] = confusionMatrix(knn_pred, valid_norm_df[,11])$overall[2]
  low_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[3,1]
  intermediate_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[2,1]
  high_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[1,1]
  best_k_df[i, 3] = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
}

kappa_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=kappa), color="red") + theme_classic()
balanced_accuray_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=balanced_accuracy), color="blue") + theme_classic()

plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")

which.max(best_k_df$kappa)#best k based on kappa
which.max(best_k_df$balanced_accuracy)#best k based on balanced accuracy
```

```{r, warning = FALSE}
#perform knn classification on the test set using best k = 1
best_knn_pred = knn(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 1)
confusionMatrix(best_knn_pred, test_norm_df[,11])#create corresponding confusion matrix 

kappa = confusionMatrix(best_knn_pred, test_norm_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Ordinal logistic regression 

```{r, warning = FALSE}
log_reg = polr(binned_quality ~ ., data = train_df)
summary(log_reg)
```

```{r, warning = FALSE}
log_prob = predict(log_reg, newdata = test_df, type = "p")

log_pred = data.frame("pred" = colnames(log_prob)[apply(log_prob,1,which.max)])
log_pred$pred = as.factor(log_pred$pred)

confusionMatrix(log_pred$pred, test_df[,11])

kappa = confusionMatrix(log_pred$pred, test_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(log_pred$pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(log_pred$pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(log_pred$pred, test_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Naive Bayes

```{r, warning = FALSE}
nb_model = naiveBayes (binned_quality ~ ., data = train_df)
nb_model
```

```{r, warning = FALSE}
nb_pred = predict(nb_model, newdata = test_df, type = "class")
confusionMatrix(nb_pred, test_df[,11])

kappa = confusionMatrix(nb_pred, test_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(nb_pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(nb_pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(nb_pred, test_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Classification trees:

(1) Full grown tree:
```{r}
set.seed(1)
tree1 <- rpart(binned_quality ~ ., data=train_df, method = "class", control = rpart.control(cp = 0, minsplit = 1, minbucket = 1, xval = 5))

# Look at the minimum standard error
printcp(tree1)
which.min(tree1$cptable[, 4])

# Take the cp associated with the minimum standard error to prune the tree:
set.seed(1)
pruned_tree1 <- prune(tree1, cp = tree1$cptable[which.min(tree1$cptable[, "xerror"]), "CP"])
prp(pruned_tree1)

# If we want to look at the most important variables:
pruned_tree1$variable.importance

# We can now predict the outcome:
pruned_tree1_pred_valid <- predict(pruned_tree1, valid_df, type="class")
pruned_tree_cm <- confusionMatrix(pruned_tree1_pred_valid, valid_df$binned_quality)
pruned_tree_cm
```

(2) Boosted tree:
```{r}
set.seed(1)
boost1 <- boosting(binned_quality ~ ., data=train_df, control = rpart.control(xval = 5))
boost1_pred_valid <- predict(boost1, valid_df, type="class")
boost1.cm <- confusionMatrix(as.factor(boost1_pred_valid$class), valid_df$binned_quality)
boost1.cm
```

(3) Bagged tree:
```{r}
set.seed(1)
bag1 <- bagging(binned_quality ~ ., data=train_df, control = rpart.control(xval = 5))
bag1_pred_valid <- predict(bag1, valid_df, type="class")
bag1.cm <- confusionMatrix(as.factor(bag1_pred_valid$class), valid_df$binned_quality)
bag1.cm
```

(4) Random Forest:
```{r}
set.seed(1)
rf1 <- randomForest(train_df[-11], train_df$binned_quality, control = rpart.control(xval = 5))
rf1_pred_valid <- predict(rf1, valid_df, type="class")
rf1.cm <- confusionMatrix(rf1_pred_valid, valid_df$binned_quality)
rf1.cm
```

### Neural Nets

(1) Determination of the best kappa with a threshold of 0.1
```{r}
nn_nodes_1 <- data.frame("Number_Nodes" = seq(from= 2, to = 10), "Kappa_Value")

for (i in 2:10){
  set.seed(1)
  nn <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = train_norm_dummy_df[, -11], hidden = i, linear.output = FALSE, threshold = 0.1)
  valid_pred_nn <- data.frame(predict(nn, valid_norm_dummy_df[, -c(11, 14)]))
  names(valid_pred_nn)[1:3] <- c("High", "Intermediate", "Low")
  valid_pred_nn$Prediction <- NA

  for(j in 1:length(valid_pred_nn$High)) {
  valid_pred_nn[j, 4] <- names(which.max(valid_pred_nn[j, 1:3]))
}

  nn_nodes_1[i-1, 2] <- confusionMatrix(as.factor(valid_pred_nn$Prediction),valid_norm_df$binned_quality)[["overall"]][["Kappa"]]
}

nn_nodes_1
```

(2) Determination of the best kappa with a threshold of 0.2
```{r}
nn_nodes_2 <- data.frame("Number_Nodes" = seq(from= 2, to = 10), "Kappa_Value" = NA)

for (i in 2:10){
  set.seed(1)
  nn <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = train_norm_dummy_df[, -11], hidden = i, linear.output = FALSE, threshold = 0.2)
  valid_pred_nn <- data.frame(predict(nn, valid_norm_dummy_df[, -c(11, 14)]))
  names(valid_pred_nn)[1:3] <- c("High", "Intermediate", "Low")
  valid_pred_nn$Prediction <- NA

  for(j in 1:length(valid_pred_nn$High)) {
  valid_pred_nn[j, 4] <- names(which.max(valid_pred_nn[j, 1:3]))
}

  nn_nodes_2[i-1, 2] <- confusionMatrix(as.factor(valid_pred_nn$Prediction),valid_norm_df$binned_quality)[["overall"]][["Kappa"]]
}

nn_nodes_2
```

(3) Best Neural Net is with threshold = 0.1 and 8 nodes:
```{r}
set.seed(1)
nn <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = train_norm_dummy_df[, -11], hidden = 8, linear.output = FALSE, threshold = 0.1)

test_pred_nn <- data.frame(predict(nn, test_norm_dummy_df[, -c(11, 14)]))
names(test_pred_nn)[1:3] <- c("High", "Intermediate", "Low")
test_pred_nn$Prediction <- NA

for(j in 1:length(test_pred_nn$High)) {
test_pred_nn[j, 4] <- names(which.max(test_pred_nn[j, 1:3]))
}

confusionMatrix(as.factor(test_pred_nn$Prediction), test_norm_df$binned_quality)
```


### Oversampled KNN 

```{r, warning = FALSE}
#initialize a new data frame with three columns: k, accuracy, and balanced_accuracy
best_k_df = data.frame(k = seq(1, 50, 1), kappa = rep(0,50), balanced_accuracy = rep(0,50))

#perform knn on the validation set using different k then store accuracy and balanced accuracy for each k in the data frame
for(i in 1:50) {
  knn_pred = knn(train = os_train_norm_df[,-11], test = valid_norm_df[,-11], cl = os_train_norm_df[,11], k = i)
  best_k_df[i, 2] = confusionMatrix(knn_pred, valid_norm_df[,11])$overall[2]
  low_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[3,1]
  intermediate_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[2,1]
  high_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[1,1]
  best_k_df[i, 3] = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
}

kappa_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=kappa), color="red") + theme_classic()
balanced_accuray_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=balanced_accuracy), color="blue") + theme_classic()

plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")

which.max(best_k_df$kappa)#best k based on accuracy
which.max(best_k_df$balanced_accuracy)#best k based on balanced accuracy
```

```{r, warning = FALSE}
#perform knn classification on the test set using best k = 2
best_knn_pred = knn(train = os_train_norm_df[,-11], test = test_norm_df[,-11], cl = os_train_norm_df[,11], k = 2)
confusionMatrix(best_knn_pred, test_norm_df[,11])#create corresponding confusion matrix 

kappa = confusionMatrix(best_knn_pred, test_norm_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(best_knn_pred, test_norm_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Oversampled ordinal logistic regression

```{r, warning = FALSE}
os_log_reg = polr(binned_quality ~ ., data = os_train_df)
summary(os_log_reg)
```

```{r, warning = FALSE}
os_log_prob = predict(os_log_reg, newdata = test_df, type = "p")

os_log_pred = data.frame("pred" = colnames(os_log_prob)[apply(os_log_prob,1,which.max)])
os_log_pred$pred = as.factor(os_log_pred$pred)

confusionMatrix(os_log_pred$pred, test_df[,11])

kappa = confusionMatrix(os_log_pred$pred, test_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Oversampled naive Bayes

```{r, warning = FALSE}
os_nb_model = naiveBayes (binned_quality ~ ., data = os_train_df)
os_nb_model
```

```{r, warning = FALSE}
os_nb_pred = predict(os_nb_model, newdata = test_df, type = "class")
confusionMatrix(os_nb_pred, test_df[,11])

kappa = confusionMatrix(os_nb_pred, test_df[,11])$overall[2]
kappa

low_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[1,1]

balanced_accuracy = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
balanced_accuracy
```

### Oversampled classification trees :

(1) Full grown tree:
```{r}
set.seed(1)
tree2 <- rpart(binned_quality ~ ., data=os_train_df, method = "class", control = rpart.control(cp = 0, minsplit = 1, minbucket = 1, xval = 5))

# Look at the minimum standard error
printcp(tree2)
which.min(tree2$cptable[, 4])

# Take the cp associated with the minimum standard error to prune the tree:
set.seed(1)
pruned_tree2 <- prune(tree2, cp = tree2$cptable[which.min(tree2$cptable[, "xerror"]), "CP"])
prp(pruned_tree2)

# If we want to look at the most important variables:
pruned_tree2$variable.importance

# We can now predict the outcome:
pruned_tree2_pred_valid <- predict(pruned_tree2, valid_df, type="class")
pruned_tree_cm <- confusionMatrix(pruned_tree2_pred_valid, valid_df$binned_quality)
pruned_tree_cm
```

(2) Boosted tree:
```{r}
set.seed(1)
boost2 <- boosting(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
boost2_pred_valid <- predict(boost2, valid_df, type="class")
boost2.cm <- confusionMatrix(as.factor(boost2_pred_valid$class), valid_df$binned_quality)
boost2.cm
```

(3) Bagged tree:
```{r}
set.seed(1)
bag2 <- bagging(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
bag2_pred_valid <- predict(bag2, valid_df, type="class")
bag2.cm <- confusionMatrix(as.factor(bag2_pred_valid$class), valid_df$binned_quality)
bag2.cm
```

(4) Random Forest:
```{r}
set.seed(1)
rf2 <- randomForest(os_train_df[-11], os_train_df$binned_quality, control = rpart.control(xval = 5))
rf2_pred_valid <- predict(rf2, valid_df, type="class")
rf2.cm <- confusionMatrix(rf2_pred_valid, valid_df$binned_quality)
rf2.cm
```


### Oversampled Neural Nets :
```{r}
set.seed(1)
nn2 <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = os_train_norm_dummy_df[, -11], hidden = 4, linear.output = FALSE, threshold = 0.2)
plot(nn1, rep="best", cex=0.8)

# valid_pred_nn1 <- compute(nn1, valid_norm_dummy_df[, -c(11:14)])$net.result
# valid_pred_nn1 <- data.frame("Prediction" = ifelse(max.col(valid_pred_nn1[ ,1:3])==1, "High", ifelse(max.col(valid_pred_nn1[ ,1:3])==2, "Intermediate", "Low")))

valid_pred_nn2 <- data.frame(predict(nn2, valid_norm_dummy_df[, -c(11, 14)]))
names(valid_pred_nn2)[1:3] <- c("High", "Intermediate", "Low")
valid_pred_nn2$Prediction <- 0

for(i in 1:length(valid_pred_nn2$High)) {
  valid_pred_nn2[i, 4] <- names(which.max(valid_pred_nn2[i, 1:3]))
}

confusionMatrix(as.factor(valid_pred_nn2$Prediction), valid_norm_df$binned_quality)
```