all_bal_acc_plot2 = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot2, all_bal_acc_plot2)
rm(list = ls())
library(naniar)
library(ggplot2)
library(reshape2)
library(magrittr)
library(dplyr)
library(car)
library(GGally)
library(viridis)
library(caret)
library(ggplot2)
library(cowplot)
library(FNN)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)
library(imbalance)
library(e1071)
library(neuralnet)
library(nnet)
# setwd("~/GitHub/CVTDM_Project")
wine = read.csv(file = "winequality-white.csv", header = T, sep = ";")
sapply(wine, function(x) length(unique(x)))
wine$binned_quality = as.factor(ifelse(wine$quality < 5, 'Low',
ifelse(wine$quality >= 5 & wine$quality < 7, "Intermediate",
ifelse(wine$quality >= 7, "High", "None"))))
wine$quality = as.factor(wine$quality)
logwine = wine[,-c(8,12)]
logwine[,-c(3,7,8,11)] = lapply(logwine[,-c(3,7,8,11)], log) #log transform all variables except citric.acid, total.sulfure.dioxide, pH and binned_quality
head(logwine)
set.seed(1)
train_index = createDataPartition(logwine$binned_quality, p = .5, list = FALSE)
train_df = logwine[train_index,]
valid_test_df = logwine[-train_index,]
valid_index = createDataPartition(valid_test_df$binned_quality, p = .6, list = FALSE)
valid_df = valid_test_df[valid_index,]
test_df = valid_test_df[-valid_index,]
#initialize normalized training and validation data frames to the original ones
train_norm_df = train_df
valid_norm_df = valid_df
test_norm_df = test_df
#use PreProcess() from the caret package and predict() to normalize numerical variables
norm_values = preProcess(train_df[,-c(11)], method = "range")
train_norm_df[,-c(11)] = predict(norm_values, train_df[,-c(11)])
valid_norm_df[,-c(11)] = predict(norm_values, valid_df[,-c(11)])
test_norm_df[,-c(11)] = predict(norm_values, test_df[,-c(11)])
# Initialize normalized training and validation data frames with dummies to the normalized ones
train_norm_dummy_df <- train_norm_df
valid_norm_dummy_df <- valid_norm_df
test_norm_dummy_df <- test_norm_df
# Creation of the vectors and implementation
train_norm_dummy_df$high_quality <- ifelse(train_norm_dummy_df$binned_quality == "High", 1, 0)
train_norm_dummy_df$intermediate_quality <- ifelse(train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
train_norm_dummy_df$low_quality <- ifelse(train_norm_dummy_df$binned_quality == "Low", 1, 0)
valid_norm_dummy_df$high_quality <- ifelse(valid_norm_dummy_df$binned_quality == "High", 1, 0)
valid_norm_dummy_df$intermediate_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
valid_norm_dummy_df$low_quality <- ifelse(valid_norm_dummy_df$binned_quality == "Low", 1, 0)
test_norm_dummy_df$high_quality <- ifelse(test_norm_dummy_df$binned_quality == "High", 1, 0)
test_norm_dummy_df$intermediate_quality <- ifelse(test_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
test_norm_dummy_df$low_quality <- ifelse(test_norm_dummy_df$binned_quality == "Low", 1, 0)
set.seed(1)
add_low_train_df = rwo(train_df, 1736, "binned_quality") # Generation of 1736 instances for 'Low'
os_train_df = rbind(train_df, add_low_train_df) # Combining the new instances to the training set
set.seed(1)
add_high_train_df = rwo(os_train_df, 1298, "binned_quality") # Generation of 1298 instances for 'High'
os_train_df = rbind(os_train_df, add_high_train_df)
os_train_norm_df = os_train_df
#use PreProcess() from the caret package and predict() to normalize numerical variables
os_norm_values = preProcess(os_train_df[,-c(11)], method = "range")
os_train_norm_df[,-c(11)] = predict(os_norm_values, os_train_df[,-c(11)])
# Initialize oversampled normalized training set with dummies to the oversampled normalized one
os_train_norm_dummy_df <- os_train_norm_df
os_train_norm_dummy_df$high_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "High", 1, 0)
os_train_norm_dummy_df$intermediate_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Intermediate", 1, 0)
os_train_norm_dummy_df$low_quality <- ifelse(os_train_norm_dummy_df$binned_quality == "Low", 1, 0)
#initialize a new data frame with three columns: k, kappa, and balanced_accuracy
best_k_df = data.frame(k = seq(1, 50, 1), kappa = rep(0,50), balanced_accuracy = rep(0,50))
#perform knn on the validation set using different k then store accuracy and balanced accuracy for each k in the data frame
for(i in 1:50) {
knn_pred = knn(train = os_train_norm_df[,-11], test = valid_norm_df[,-11], cl = os_train_norm_df[,11], k = i)
best_k_df[i, 2] = confusionMatrix(knn_pred, valid_norm_df[,11])$overall[2]
low_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[1,1]
best_k_df[i, 3] = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
}
kappa_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=kappa), color="red") + theme_classic()
balanced_accuray_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=balanced_accuracy), color="blue") + theme_classic()
plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")
which.max(best_k_df$kappa)#best k based on kappa
which.max(best_k_df$balanced_accuracy)#best k based on balanced accuracy
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 1, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
os_log_reg = multinom(binned_quality ~ ., data = os_train_df)
summary(os_log_reg)
os_log_prob = predict(os_log_reg, newdata = test_df, type = "p")
os_log_pred = data.frame("pred" = colnames(os_log_prob)[apply(os_log_prob,1,which.max)])
os_log_pred$pred = as.factor(os_log_pred$pred)
confusionMatrix(os_log_pred$pred, test_df[,11])
kappa_logist = confusionMatrix(os_log_pred$pred, test_df[,11])$overall[2]
kappa_logist
low_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(os_log_pred$pred, test_df[,11])$byClass[1,1]
bal_acc_logist = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_logist
os_nb_model = naiveBayes (binned_quality ~ ., data = os_train_df)
os_nb_model
os_nb_pred = predict(os_nb_model, newdata = test_df, type = "class")
os_nb_prob = predict(os_nb_model, newdata = test_df, type = "raw")
confusionMatrix(os_nb_pred, test_df[,11])
kappa_bayes = confusionMatrix(os_nb_pred, test_df[,11])$overall[2]
kappa_bayes
low_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(os_nb_pred, test_df[,11])$byClass[1,1]
bal_acc_bayes = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_bayes
set.seed(1)
tree1 <- rpart(binned_quality ~ ., data=os_train_df, method = "class", control = rpart.control(cp = 0, minsplit = 1, minbucket = 1, xval = 5))
# Look at the minimum standard error
printcp(tree1)
which.min(tree1$cptable[, 4])
# Take the cp associated with the minimum standard error to prune the tree:
set.seed(1)
pruned_tree1 <- prune(tree1, cp = tree1$cptable[which.min(tree1$cptable[, "xerror"]), "CP"])
prp(pruned_tree1)
# If we want to look at the most important variables:
pruned_tree1$variable.importance
# We can now predict the outcome:
pruned_tree1_pred_test <- predict(pruned_tree1, test_df, type="class")
pruned_tree1_prob_test <- predict(pruned_tree1, test_df, type="prob")
pruned_tree1_cm <- confusionMatrix(pruned_tree1_pred_test, test_df$binned_quality)
pruned_tree1_cm
kappa_pruned_tree1 = pruned_tree1_cm$overall[2]
kappa_pruned_tree1
low_sensitivity = pruned_tree1_cm$byClass[3,1]
intermediate_sensitivity = pruned_tree1_cm$byClass[2,1]
high_sensitivity = pruned_tree1_cm$byClass[1,1]
bal_acc_pruned_tree1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_pruned_tree1
set.seed(1)
boost1 <- boosting(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
boost1_pred_test <- predict(boost1, test_df, type="class")
boost1_prob_test <- predict(boost1, test_df, type="prob")
boost1.cm <- confusionMatrix(as.factor(boost1_pred_test$class), test_df$binned_quality)
boost1.cm
kappa_boost1 = boost1.cm$overall[2]
kappa_boost1
low_sensitivity = boost1.cm$byClass[3,1]
intermediate_sensitivity = boost1.cm$byClass[2,1]
high_sensitivity = boost1.cm$byClass[1,1]
bal_acc_boost1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_boost1
set.seed(1)
bag1 <- bagging(binned_quality ~ ., data=os_train_df, control = rpart.control(xval = 5))
bag1_pred_test <- predict(bag1, test_df, type="class")
bag1_prob_test <- predict(bag1, test_df, type="prob")
bag1.cm <- confusionMatrix(as.factor(bag1_pred_test$class), test_df$binned_quality)
bag1.cm
kappa_bag1 = bag1.cm$overall[2]
kappa_bag1
low_sensitivity = bag1.cm$byClass[3,1]
intermediate_sensitivity = bag1.cm$byClass[2,1]
high_sensitivity = bag1.cm$byClass[1,1]
bal_acc_bag1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_bag1
set.seed(1)
rf1 <- randomForest(os_train_df[-11], os_train_df$binned_quality, control = rpart.control(xval = 5))
rf1_pred_test <- predict(rf1, test_df, type="class")
rf1_prob_test <- predict(rf1, test_df, type="prob")
rf1.cm <- confusionMatrix(rf1_pred_test, test_df$binned_quality)
rf1.cm
kappa_rf1 = rf1.cm$overall[2]
kappa_rf1
low_sensitivity = rf1.cm$byClass[3,1]
intermediate_sensitivity = rf1.cm$byClass[2,1]
high_sensitivity = rf1.cm$byClass[1,1]
bal_acc_rf1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_rf1
set.seed(1)
nn1 <- neuralnet(high_quality + intermediate_quality + low_quality ~ ., data = train_norm_dummy_df[, -11], hidden = 4, linear.output = FALSE, threshold = 0.5)
test_pred_nn1 <- data.frame(predict(nn1, test_norm_dummy_df[, -c(11, 12, 13, 14)]))
names(test_pred_nn1)[1:3] <- c("High", "Intermediate", "Low")
test_pred_nn1$Prediction <- NA
for(j in 1:length(test_pred_nn1$High)) {
test_pred_nn1[j, 4] <- names(which.max(test_pred_nn1[j, 1:3]))
}
nn1.cm <- confusionMatrix(as.factor(test_pred_nn1$Prediction), test_norm_df$binned_quality)
nn1.cm
kappa_nn1 = nn1.cm$overall[2]
kappa_nn1
low_sensitivity = nn1.cm$byClass[3,1]
intermediate_sensitivity = nn1.cm$byClass[2,1]
high_sensitivity = nn1.cm$byClass[1,1]
bal_acc_nn1 = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_nn1
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
all_kappa_plot = ggplot(data = results_df, aes(x = reorder(method, kappa))) +
geom_bar(aes(y = kappa), stat = "identity") +
labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
theme(axis.text.x = element_text(angle=90))
all_bal_acc_plot = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot, all_bal_acc_plot)
ensemble_prob_low_df = data.frame("knn_prob_low" = attr(best_knn_pred, "prob")[,3],
"log_prob_low" = os_log_prob[,3],
"nb_prob_low" = os_nb_prob[,3],
"tree_prob_low" = pruned_tree1_prob_test[,3],
"boosting_prob_low" = boost1_prob_test$prob[,3],
"bagging_prob_low" = bag1_prob_test$prob[,3],
"rf_prob_low" = rf1_prob_test[,3],
"nnet_prob_low" = test_pred_nn1[,3])
ensemble_prob_low_df$mean_prob_low = apply(ensemble_prob_low_df[,c(2,3,5,7)], 1, mean)
head(ensemble_prob_low_df)
ensemble_prob_inter_df = data.frame("knn_prob_inter" = attr(best_knn_pred, "prob")[,2],
"log_prob_inter" = os_log_prob[,2],
"nb_prob_inter" = os_nb_prob[,2],
"tree_prob_inter" = pruned_tree1_prob_test[,2],
"boosting_prob_inter" = boost1_prob_test$prob[,2],
"bagging_prob_inter" = bag1_prob_test$prob[,2],
"rf_prob_inter" = rf1_prob_test[,2],
"nnet_prob_inter" = test_pred_nn1[,2])
ensemble_prob_inter_df$mean_prob_inter = apply(ensemble_prob_inter_df[,c(2,3,5,7)], 1, mean)
head(ensemble_prob_inter_df)
ensemble_prob_high_df = data.frame("knn_prob_high" = attr(best_knn_pred, "prob")[,1],
"log_prob_high" = os_log_prob[,1],
"nb_prob_high" = os_nb_prob[,1],
"tree_prob_high" = pruned_tree1_prob_test[,1],
"boosting_prob_high" = boost1_prob_test$prob[,1],
"bagging_prob_high" = bag1_prob_test$prob[,1],
"rf_prob_high" = rf1_prob_test[,1],
"nnet_prob_high" = test_pred_nn1[,1])
ensemble_prob_high_df$mean_prob_high = apply(ensemble_prob_high_df[,c(2,3,5,7)], 1, mean)
head(ensemble_prob_high_df)
ensemble_prob_df = data.frame("Low" = ensemble_prob_low_df$mean_prob_low,
"Intermediate" = ensemble_prob_inter_df$mean_prob_inter,
"High" = ensemble_prob_high_df$mean_prob_high)
ensemble_prob_df$mean_prob_pred = as.factor(colnames(ensemble_prob_df)[apply(ensemble_prob_df,1,which.max)])
head(ensemble_prob_df)
ensembles_pred_df = data.frame("actual_value" = test_df$binned_quality,
"knn_pred" = best_knn_pred,
"log_pred" = os_log_pred$pred,
"nb_pred" = os_nb_pred,
"tree_pred" = pruned_tree1_pred_test,
"boosting_pred" = as.factor(boost1_pred_test$class),
"bagging_pred" = as.factor(bag1_pred_test$class),
"rf_pred" = rf1_pred_test,
"nnet_pred" = as.factor(test_pred_nn1$Prediction))
ensembles_pred_df$majority_vote_pred = as.factor(apply(ensembles_pred_df[,c(3,4,6,8)], 1, function(x) names(which.max(table(x)))))
ensembles_pred_df$mean_prob_pred = ensemble_prob_df$mean_prob_pred
head(ensembles_pred_df)
confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])
kappa_majority = confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])$overall[2]
kappa_majority
low_sensitivity = confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(ensembles_pred_df$majority_vote_pred, test_df[,11])$byClass[1,1]
bal_acc_majority = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_majority
confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])
kappa_mean_prob = confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])$overall[2]
kappa_mean_prob
low_sensitivity = confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(ensembles_pred_df$mean_prob_pred, test_df[,11])$byClass[1,1]
bal_acc_mean_prob = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_mean_prob
results_df[9,] = c("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
all_kappa_plot2 = ggplot(data = results_df, aes(x = reorder(method, kappa))) +
geom_bar(aes(y = kappa), stat = "identity") +
labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
theme(axis.text.x = element_text(angle=90))
all_bal_acc_plot2 = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot2, all_bal_acc_plot2)
all_kappa_plot = ggplot(data = results_df, aes(x = reorder(method, kappa))) +
geom_bar(aes(y = kappa), stat = "identity") +
labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
theme(axis.text.x = element_text(angle=90))
all_bal_acc_plot = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot, all_bal_acc_plot)
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
all_kappa_plot = ggplot(data = results_df, aes(x = reorder(method, kappa))) +
geom_bar(aes(y = kappa), stat = "identity") +
labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
theme(axis.text.x = element_text(angle=90))
all_bal_acc_plot = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot, all_bal_acc_plot)
results_df
results_df[9,] = c("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
bal_acc_majority
results_df[9,] = c("majority vote", kappa_majority, round(bal_acc_majority,7))
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df
results_df[9,] = c("majority vote", round(kappa_majority,3), round(bal_acc_majority,3))
results_df
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
results_df[9,] = c("majority vote", round(kappa_majority,7), round(bal_acc_majority,7))
results_df[10,] = c("mean probability", round(kappa_mean_prob,7), round(bal_acc_mean_prob,7))
results_df
results_df[9,] = c("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df[9,] = c("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df[,c(2,3)] = round(results_df[,c(2,3)],7)
str(results_df)
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
str(results_df)
type(bal_acc_majority)
results_df[9,] = c("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = c("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df[,c(2,3)] = as.numeric(results_df[,c(2,3)])
results_df = data.frame("method" = c("knn", "logistic regression", "naive Bayes", "classification tree", "boosted tree", "bagged tree", "random forest", "neural net"),
"kappa" = c(kappa_best_knn, kappa_logist, kappa_bayes, kappa_pruned_tree1, kappa_boost1, kappa_bag1, kappa_rf1, kappa_nn1),
"balanced_accuracy" = c(bal_acc_best_knn, bal_acc_logist, bal_acc_bayes, bal_acc_pruned_tree1, bal_acc_boost1, bal_acc_bag1, bal_acc_rf1, bal_acc_nn1))
results_df
results_df[9,] = list("majority vote", kappa_majority, bal_acc_majority)
results_df[10,] = list("mean probability", kappa_mean_prob, bal_acc_mean_prob)
results_df
all_kappa_plot2 = ggplot(data = results_df, aes(x = reorder(method, kappa))) +
geom_bar(aes(y = kappa), stat = "identity") +
labs(title = "Kappa for each method", x = "Method", y = "Kappa") +
theme(axis.text.x = element_text(angle=90))
all_bal_acc_plot2 = ggplot(data = results_df, aes(x = reorder(method, balanced_accuracy))) +
geom_bar(aes(y = balanced_accuracy), stat = "identity") +
labs(title = "Balanced accuracy for each method", x = "Method", y = "Balanced accuracy") +
theme(axis.text.x = element_text(angle=90))
plot_grid(all_kappa_plot2, all_bal_acc_plot2)
#initialize a new data frame with three columns: k, kappa, and balanced_accuracy
best_k_df = data.frame(k = seq(1, 50, 1), kappa = rep(0,50), balanced_accuracy = rep(0,50))
#perform knn on the validation set using different k then store accuracy and balanced accuracy for each k in the data frame
for(i in 1:50) {
knn_pred = knn(train = os_train_norm_df[,-11], test = valid_norm_df[,-11], cl = os_train_norm_df[,11], k = i)
best_k_df[i, 2] = confusionMatrix(knn_pred, valid_norm_df[,11])$overall[2]
low_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(knn_pred, valid_norm_df[,11])$byClass[1,1]
best_k_df[i, 3] = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
}
kappa_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=kappa), color="red") + theme_classic()
balanced_accuray_plot = ggplot(data= best_k_df) + geom_line(aes(x=k,y=balanced_accuracy), color="blue") + theme_classic()
plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")
which.max(best_k_df$kappa)#best k based on kappa
which.max(best_k_df$balanced_accuracy)#best k based on balanced accuracy
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 28, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 1, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 28, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
plot_grid(kappa_plot, balanced_accuray_plot, ncol = 1, align = "v")
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 5, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 1, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 6, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 4, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 3, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 7, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 6, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
#perform knn classification on the test set using best k = 1
set.seed(1)
best_knn_pred = knn3Train(train = train_norm_df[,-11], test = test_norm_df[,-11], cl = train_norm_df[,11], k = 1, prob = T)
confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])#create corresponding confusion matrix
kappa_best_knn = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$overall[2]
kappa_best_knn
low_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[3,1]
intermediate_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[2,1]
high_sensitivity = confusionMatrix(as.factor(best_knn_pred), test_norm_df[,11])$byClass[1,1]
bal_acc_best_knn = (low_sensitivity + intermediate_sensitivity + high_sensitivity) / 3
bal_acc_best_knn
